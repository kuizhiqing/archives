# Dec

* [LLM Visualization](https://bbycroft.net/llm)
* [Common Go Mistakes](https://100go.co/)

## Mamba SSM

* State Space Model, still fell short of Transformers, no model has actually matched a well-tuned modern Transformer.
* Scale to long context, faster
* selective technique
* [arxiv](https://arxiv.org/abs/2312.00752)
* [github](https://github.com/state-spaces/mamba)
